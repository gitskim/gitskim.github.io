{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa6e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3a0ae",
   "metadata": {},
   "source": [
    "You have a matrix $X$ of 100 samples of 100 input features, and a vector $Y$ of 100 outputs. Using this training data, you want to fit a linear model that will make good predictions of $y$ given new $x$s, $\\hat y = a^T x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a58c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation code -- run this cell, but you don't have to read it\n",
    "pts = 100\n",
    "dims = 100\n",
    "a0 = np.zeros(dims)\n",
    "a0[[11, 71]] = 1, -1\n",
    "\n",
    "def make_data(pts, dims, a0):\n",
    "    X = np.random.normal(0, 10, (pts, dims))\n",
    "    y0 = X @ a0\n",
    "    noise = np.random.normal(0, 1, pts)\n",
    "    Y = y0 + noise\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = make_data(pts, dims, a0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e7496",
   "metadata": {},
   "source": [
    "**Question 1:** if you fit $a$ using all of $X$ and $Y$ and no regularization,  how good will the fit to the **training** data set be? Why?\n",
    "\n",
    "**Answer:** very good! there are as many features as data points, so you can even invert the training data matrix to get a perfect fit.\n",
    "\n",
    "**Question 2:** How well do you expect your model will perform on new, unseen data drawn from the same distribution? Why?\n",
    "\n",
    "**Answer:** very bad, since the high number of features and low number of data points leads to tons of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7fd4c",
   "metadata": {},
   "source": [
    "**Question 3:** The below function fits a linear model using least squares and L1 regularization (lasso regression). How can you use this to produce a model that will generalize well? Why does regularization improve generalization?\n",
    "\n",
    "**Answer:** Fit to training data set with appropriate regularization strength. This works because regularization forces the model to produce a simpler model, which rejects noise in the training data even at the expense of higher training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984cfe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_lasso(X, Y, c=1e-6):\n",
    "    lr = sklearn.linear_model.Lasso(\n",
    "        alpha=c, fit_intercept=False).fit(X, Y).coef_\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd432b",
   "metadata": {},
   "source": [
    "**Question 4:** Using whatever technique you think is appropriate, write code to find the amount of regularization that will give the best generalization. Use it to find the best regularization parameter and fit a model with that parameter.\n",
    "\n",
    "**Answer:** One way is with cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292af6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(a, X, Y):\n",
    "    return np.mean((X@a - Y)**2)\n",
    "\n",
    "def k_fold_cross_validation(X, Y, k=3, c=1e-6):\n",
    "    start = 0\n",
    "    total = len(X)\n",
    "    errors = []\n",
    "    for i in range(k):\n",
    "        val_X = X[start:start+(total//k)]\n",
    "        train_X = np.concatenate([\n",
    "            X[:start], X[start+(total//k):]\n",
    "        ])\n",
    "        val_Y = Y[start:start+(total//k)]\n",
    "        train_Y = np.concatenate([\n",
    "            Y[:start], Y[start+(total//k):]\n",
    "        ])\n",
    "        a = linreg_lasso(train_X, train_Y, c)\n",
    "        error = compute_mse(a, val_X, val_Y)\n",
    "        errors.append(error)\n",
    "        start = start+(total//k)\n",
    "        \n",
    "    return np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f173aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = np.logspace(-1, 2, 10, base=10)\n",
    "error_cv = [\n",
    "    k_fold_cross_validation(X_train, Y_train, 3, c)\n",
    "    for c in cc\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f1f59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.1       ,   0.21544347,   0.46415888,   1.        ,\n",
       "          2.15443469,   4.64158883,  10.        ,  21.5443469 ,\n",
       "         46.41588834, 100.        ]),\n",
       " [3.019965383929488,\n",
       "  2.4280967710414196,\n",
       "  1.812653424362381,\n",
       "  1.4175558422978771,\n",
       "  1.3017527898034045,\n",
       "  1.5628763081766586,\n",
       "  3.137640480887894,\n",
       "  10.341882182124822,\n",
       "  43.554432361273605,\n",
       "  169.5827501135839])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc, error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb489d47",
   "metadata": {},
   "source": [
    "Looks like C = 2 or so will do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e82460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
